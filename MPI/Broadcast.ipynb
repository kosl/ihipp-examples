{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f166b008",
   "metadata": {},
   "source": [
    "## Broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b46418c",
   "metadata": {},
   "source": [
    "Complete the program. \n",
    "\n",
    "1. Add the `MPI_Bcast` routine to broadcast an array with 10.000.000 numbers from process with rank 0. \n",
    "\n",
    "2. Write your own broadcast function `my_Bcast` using `MPI_Send` and `MPI_Recv` routines. \n",
    "\n",
    "3. Measure the time of both routines using MPI function `MPI_Wtime` when running with 2, 4, 8 processors. What do you make of the differences at different scales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228cbcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "?MPI::MPI_Bcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0751738",
   "metadata": {},
   "outputs": [],
   "source": [
    "?MPI::MPI_Wtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d911644e",
   "metadata": {},
   "source": [
    "***\n",
    "#### C skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f14da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file bcast.c\n",
    "#include <stdio.h>\n",
    "#include <mpi.h>\n",
    "\n",
    "void my_Bcast(void* data, int count, MPI_Datatype datatype, int root, MPI_Comm communicator) \n",
    "{\n",
    "    int rank, size, i;\n",
    "    MPI_Comm_rank(communicator, &rank);\n",
    "    MPI_Comm_size(communicator, &size);\n",
    "    \n",
    "    // TODO:\n",
    "    // If we are the root process, send our data to everyone\n",
    "    // If we are a receiver process, receive the data from the root\n",
    "    \n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    int rank, i;\n",
    "    int num_elements = 10000000; // size of array\n",
    "    int num_trials = 10; // number of timing experiments\n",
    "\n",
    "    MPI_Init(NULL, NULL);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "\n",
    "    double total_my_bcast_time = 0.0;\n",
    "    double total_mpi_bcast_time = 0.0;\n",
    "    int* data = (int*)malloc(sizeof(int) * num_elements); // create array\n",
    "\n",
    "    for (i = 0; i < num_trials; i++) {\n",
    "        // TODO:\n",
    "        // broadcast with MPI_Bcast\n",
    "        // time MPI_Bcast\n",
    "        // synchronize before starting timing and before obtaining final time\n",
    "\n",
    "        // TODO:\n",
    "        // broadcast with my_Bcast\n",
    "        // time my_Bcast\n",
    "\n",
    "    }\n",
    "\n",
    "    // Print resulting times\n",
    "    if (rank == 0) {\n",
    "        printf(\"Avg my_Bcast time = %lf\\n\", total_my_bcast_time / num_trials);\n",
    "        printf(\"Avg MPI_Bcast time = %lf\\n\", total_mpi_bcast_time / num_trials);\n",
    "    }\n",
    "\n",
    "    free(data);\n",
    "    MPI_Finalize();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4a1d57",
   "metadata": {},
   "source": [
    "Now compile it and run it with 4 processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36f8be0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mpicc bcast.c -o bcast && mpirun -np 2 --allow-run-as-root bcast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d523ee",
   "metadata": {},
   "source": [
    "***\n",
    "#### Python skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7596fb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file bcast.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "def my_Bcast(root):\n",
    "    global data\n",
    "    comm = MPI.COMM_WORLD\n",
    "    rank = comm.Get_rank()\n",
    "    size = comm.Get_size()\n",
    "    # TODO:\n",
    "    # If we are the root process, send our data to everyone\n",
    "    # If we are a receiver process, receive the data from the root\n",
    "        \n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "num_elements = 10000000\n",
    "num_trials = 10\n",
    "        \n",
    "total_my_bcast_time = 0.0\n",
    "total_mpi_bcast_time = 0.0\n",
    "data = [None] * num_elements\n",
    "\n",
    "for i in range(0, num_trials):\n",
    "    pass\n",
    "    # TODO:\n",
    "    # broadcast with MPI_Bcast\n",
    "    # time MPI_Bcast\n",
    "    # synchronize before starting timing and obtaining final time\n",
    "    \n",
    "    # TODO:\n",
    "    # broadcast with my_Bcast\n",
    "    # time my_Bcast\n",
    "\n",
    "# print resulting times\n",
    "if rank == 0:\n",
    "    print(\"Avg my_Bcast time = %lf\" % (total_my_bcast_time / num_trials))\n",
    "    print(\"Avg MPI_Bcast time = %lf\" % (total_mpi_bcast_time / num_trials))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15f4b77",
   "metadata": {},
   "source": [
    "Now compile it and run it with 4 processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12057c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -np 2 --allow-run-as-root python bcast.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9936fe22",
   "metadata": {},
   "source": [
    "***\n",
    "#### Fortran skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcc496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file bcast.f90\n",
    "subroutine my_Bcast(data, count, datatype, root, communicator)\n",
    "    double precision, dimension (num_elements) :: data\n",
    "    integer :: count, root\n",
    "    integer :: datatype, communicator\n",
    "    \n",
    "    integer ( kind = 4 ) error\n",
    "    integer ( kind = 4 ) rank, size\n",
    "    integer :: i\n",
    "    call MPI_Comm_rank(communicator, rank, error)\n",
    "    call MPI_Comm_size(communicator, size, error)\n",
    "    \n",
    "    ! TODO:\n",
    "    ! If we are the root process, send our data to everyone\n",
    "    ! If we are a receiver process, receive the data from the root\n",
    "    \n",
    "end\n",
    "\n",
    "program bcast\n",
    "use mpi\n",
    "\n",
    "integer ( kind = 4 ) error\n",
    "integer ( kind = 4 ) rank\n",
    "integer :: i, num_elements, num_trials\n",
    "double precision :: total_my_bcast_time, total_mpi_bcast_time\n",
    "double precision, allocatable, dimension (:) :: data\n",
    "num_elements = 10000000\n",
    "num_trials = 10\n",
    "    \n",
    "call MPI_Init(error)\n",
    "call MPI_Comm_rank(MPI_COMM_WORLD, rank, error)\n",
    "\n",
    "total_my_bcast_time = 0\n",
    "total_mpi_bcast_time = 0\n",
    "allocate(data(0:num_elements-1)) ! create array\n",
    "\n",
    "do i = 0, num_trials-1\n",
    "    ! TODO:\n",
    "    ! broadcast with MPI_Bcast\n",
    "    ! time MPI_Bcast\n",
    "    ! synchronize before starting timing\n",
    "    \n",
    "    ! TODO:\n",
    "    ! broadcast with my_Bcast\n",
    "    ! time my_Bcast\n",
    "end do\n",
    "\n",
    "! print resulting times\n",
    "if (rank .eq. 0) then\n",
    "    print *, \"Avg my_Bcast time = \", (total_my_bcast_time / num_trials)\n",
    "    print *, \"Avg MPI_Bcast time = \", (total_mpi_bcast_time / num_trials)\n",
    "end if\n",
    "\n",
    "call MPI_Finalize(error)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f56ea58",
   "metadata": {},
   "source": [
    "Now compile it and run it with 4 processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b2ab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpif90 bcast.f90 && mpirun -np 2 --allow-run-as-root a.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d6e30d",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeebe3d",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6088d5ce",
   "metadata": {},
   "source": [
    "### You can compare with our solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d3fdb7",
   "metadata": {},
   "source": [
    "***\n",
    "#### C solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3064af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file bcast.c\n",
    "#include <stdio.h>\n",
    "#include <mpi.h>\n",
    "\n",
    "void my_Bcast(void* data, int count, MPI_Datatype datatype, int root, MPI_Comm communicator) \n",
    "{\n",
    "    int rank, size, i;\n",
    "    MPI_Comm_rank(communicator, &rank);\n",
    "    MPI_Comm_size(communicator, &size);\n",
    "\n",
    "    // If we are the root process, send our data to everyone\n",
    "    if (rank == root) {\n",
    "        for (i = 0; i < size; i++) {\n",
    "            if (i != rank) {\n",
    "                MPI_Send(data, count, datatype, i, 0, communicator);\n",
    "            }\n",
    "        }\n",
    "    } else {\n",
    "    // If we are a receiver process, receive the data from the root\n",
    "        MPI_Recv(data, count, datatype, root, 0, communicator, MPI_STATUS_IGNORE);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    int rank, i;\n",
    "    int num_elements = 10000000; // size of array\n",
    "    int num_trials = 10;\n",
    "\n",
    "    MPI_Init(NULL, NULL);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "\n",
    "    double total_my_bcast_time = 0.0;\n",
    "    double total_mpi_bcast_time = 0.0;\n",
    "    int* data = (int*)malloc(sizeof(int) * num_elements); // create array\n",
    "\n",
    "    for (i = 0; i < num_trials; i++) {\n",
    "        // Time MPI_Bcast\n",
    "        // Synchronize before starting timing\n",
    "        MPI_Barrier(MPI_COMM_WORLD);\n",
    "        total_mpi_bcast_time -= MPI_Wtime();\n",
    "        MPI_Bcast(data, num_elements, MPI_INT, 0, MPI_COMM_WORLD);\n",
    "        // Synchronize again before obtaining final time\n",
    "        MPI_Barrier(MPI_COMM_WORLD);\n",
    "        total_mpi_bcast_time += MPI_Wtime();\n",
    "\n",
    "        // Time my_Bcast\n",
    "        MPI_Barrier(MPI_COMM_WORLD);\n",
    "        total_my_bcast_time -= MPI_Wtime();\n",
    "        my_Bcast(data, num_elements, MPI_INT, 0, MPI_COMM_WORLD);\n",
    "        MPI_Barrier(MPI_COMM_WORLD);\n",
    "        total_my_bcast_time += MPI_Wtime();\n",
    "    }\n",
    "\n",
    "    // Print resulting times\n",
    "    if (rank == 0) {\n",
    "        printf(\"Avg my_Bcast time = %lf\\n\", total_my_bcast_time / num_trials);\n",
    "        printf(\"Avg MPI_Bcast time = %lf\\n\", total_mpi_bcast_time / num_trials);\n",
    "    }\n",
    "\n",
    "    free(data);\n",
    "    MPI_Finalize();\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc5bc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpicc bcast.c -o bcast && mpirun -np 2 --allow-run-as-root bcast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a467c8f8",
   "metadata": {},
   "source": [
    "***\n",
    "#### Python solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304966f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file bcast.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "def my_Bcast(root):\n",
    "    global data\n",
    "    comm = MPI.COMM_WORLD\n",
    "    rank = comm.Get_rank()\n",
    "    size = comm.Get_size()\n",
    "    \n",
    "    # If we are the root process, send our data to everyone\n",
    "    if rank == root:\n",
    "        for i in range(0, size):\n",
    "            if i != rank:\n",
    "                comm.send(data, dest=i)\n",
    "    else:\n",
    "    # If we are a receiver process, receive the data from the root\n",
    "        data = comm.recv(source=0)\n",
    "        \n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "num_elements = 10000000\n",
    "num_trials = 10\n",
    "        \n",
    "total_my_bcast_time = 0.0\n",
    "total_mpi_bcast_time = 0.0\n",
    "data = [None] * num_elements\n",
    "\n",
    "for i in range(0, num_trials):\n",
    "    # Time MPI_Bcast\n",
    "    # Synchronize before starting timing\n",
    "    comm.Barrier()\n",
    "    total_mpi_bcast_time -= MPI.Wtime()\n",
    "    data = comm.bcast(data, root=0)\n",
    "    # Synchronize again before obtaining final time\n",
    "    comm.Barrier()\n",
    "    total_mpi_bcast_time += MPI.Wtime()\n",
    "        \n",
    "    # Time my_Bcast\n",
    "    comm.Barrier()\n",
    "    total_my_bcast_time -= MPI.Wtime()\n",
    "    my_Bcast(0)\n",
    "    comm.Barrier()\n",
    "    total_my_bcast_time += MPI.Wtime()\n",
    "\n",
    "# Print resulting times\n",
    "if rank == 0:\n",
    "    print(\"Avg my_Bcast time = %lf\" % (total_my_bcast_time / num_trials))\n",
    "    print(\"Avg MPI_Bcast time = %lf\" % (total_mpi_bcast_time / num_trials))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb648087",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -np 2 --allow-run-as-root python bcast.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6976409a",
   "metadata": {},
   "source": [
    "***\n",
    "#### Fortran solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86652edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file bcast.f90\n",
    "subroutine my_Bcast(data, count, datatype, root, communicator)\n",
    "    double precision, dimension (num_elements) :: data\n",
    "    integer :: count, root\n",
    "    integer :: datatype, communicator\n",
    "    \n",
    "    integer ( kind = 4 ) error\n",
    "    integer ( kind = 4 ) rank, size\n",
    "    integer :: i\n",
    "    call MPI_Comm_rank(communicator, rank, error)\n",
    "    call MPI_Comm_size(communicator, size, error)\n",
    "    \n",
    "    ! If we are the root process, send our data to everyone\n",
    "    if (rank .eq. root) then\n",
    "        do i = 0, size-1\n",
    "            if (i /= rank) then\n",
    "                call MPI_Send(data, count, datatype, i, 0, communicator, error)\n",
    "            end if\n",
    "        end do\n",
    "    else\n",
    "        ! If we are a receiver process, receive the data from the root\n",
    "        call MPI_Recv(data, count, datatype, root, 0, communicator, MPI_STATUS_IGNORE, error)\n",
    "    end if   \n",
    "end\n",
    "\n",
    "program bcast\n",
    "use mpi\n",
    "\n",
    "integer ( kind = 4 ) error\n",
    "integer ( kind = 4 ) rank\n",
    "integer :: i, num_elements, num_trials\n",
    "double precision :: total_my_bcast_time, total_mpi_bcast_time\n",
    "double precision, allocatable, dimension (:) :: data\n",
    "num_elements = 10000000\n",
    "num_trials = 10\n",
    "    \n",
    "call MPI_Init(error)\n",
    "call MPI_Comm_rank(MPI_COMM_WORLD, rank, error)\n",
    "\n",
    "total_my_bcast_time = 0\n",
    "total_mpi_bcast_time = 0\n",
    "allocate(data(0:num_elements-1)) ! create array\n",
    "\n",
    "do i = 0, num_trials-1\n",
    "    ! Time MPI_Bcast\n",
    "    ! Synchronize before starting timing\n",
    "    call MPI_Barrier(MPI_COMM_WORLD, error)\n",
    "    total_mpi_bcast_time = total_mpi_bcast_time - MPI_Wtime()\n",
    "    call MPI_Bcast(data, num_elements, MPI_INT, 0, MPI_COMM_WORLD, error)\n",
    "    ! Synchronize again before obtaining final time\n",
    "    call MPI_Barrier(MPI_COMM_WORLD, error)\n",
    "    total_mpi_bcast_time = total_mpi_bcast_time + MPI_Wtime()   \n",
    "    \n",
    "    ! Time my_Bcast\n",
    "    call MPI_Barrier(MPI_COMM_WORLD, error)\n",
    "    total_my_bcast_time = total_my_bcast_time - MPI_Wtime()\n",
    "    call my_Bcast(data, num_elements, MPI_INT, 0, MPI_COMM_WORLD)\n",
    "    call MPI_Barrier(MPI_COMM_WORLD, error)\n",
    "    total_my_bcast_time = total_my_bcast_time + MPI_Wtime()\n",
    "end do\n",
    "\n",
    "! print resulting times\n",
    "if (rank .eq. 0) then\n",
    "    print *, \"Avg my_Bcast time = \", (total_my_bcast_time / num_trials)\n",
    "    print *, \"Avg MPI_Bcast time = \", (total_mpi_bcast_time / num_trials)\n",
    "end if\n",
    "\n",
    "call MPI_Finalize(error)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d4acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpif90 bcast.f90 && mpirun -np 2 --allow-run-as-root a.out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++14 with OpenMP and MPI",
   "language": "C++14",
   "name": "xcpp14"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
