{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f166b008",
   "metadata": {},
   "source": [
    "## One sided communication in a ring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3b534d",
   "metadata": {},
   "source": [
    "In this exercise the goal is to substitute nonblocking communication with one sided communication. \n",
    "\n",
    "What you need to do is create a window for the receive buffer and substitute the sending and receiving by calling MPI_Put on the process that previously called MPI_Send. Also don't forget to do synchronization with MPI_Win_fence. \n",
    "\n",
    "1. Fill out the skeleton to create all `rcv_buf` as windows in their processes. Don't forget to free the window when you are done. \n",
    "\n",
    "2. Substitute the Issend/Recv/Wait with Win_fence/Put/Win_fence sequence. \n",
    "\n",
    "* There are two solutions to substituting nonblocking communication with one-sided communication. Do you have any idea, why would we preffer using MPI_Put instead of MPI_Get? What is your preferred way, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61763682",
   "metadata": {},
   "outputs": [],
   "source": [
    "?MPI::MPI_Win_create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b789f2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "?MPI::MPI_Put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d3b1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "?MPI::MPI_Get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e29849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "?MPI::MPI_Win_fence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c94ad0",
   "metadata": {},
   "source": [
    "***\n",
    "#### C skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a30586",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ring.c\n",
    "#include <stdio.h>\n",
    "#include <mpi.h>\n",
    "\n",
    "int main()\n",
    "{\n",
    "    int rank, size;\n",
    "    int snd_buf, rcv_buf;\n",
    "    int right, left;\n",
    "    int sum, i;\n",
    "    MPI_Status  status;\n",
    "    MPI_Request request;\n",
    "\n",
    "    ___________ win;\n",
    "\n",
    "    MPI_Init(NULL, NULL);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "\n",
    "    right = (rank+1)      % size;\n",
    "    left  = (rank-1+size) % size;\n",
    "\n",
    "    /* Create the window. */\n",
    "    MPI_Win_create(____________________________________, &win);\n",
    "\n",
    "    sum = 0;\n",
    "    snd_buf = rank;\n",
    "\n",
    "    for(i = 0; i < size; i++) \n",
    "    {\n",
    "        MPI_Issend(&snd_buf, 1, MPI_INT, right, 17, MPI_COMM_WORLD, &request);\n",
    "        MPI_Recv  (&rcv_buf, 1, MPI_INT, left,  17, MPI_COMM_WORLD, &status);\n",
    "        MPI_Wait(&request, &status);\n",
    "\n",
    "        snd_buf = rcv_buf;\n",
    "        sum += rcv_buf;\n",
    "    }\n",
    "\n",
    "    printf(\"PE%i:\\tSum = %i\\n\", rank, sum);\n",
    "\n",
    "    MPI_Finalize();\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdce7fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpicc ring.c && mpirun -np 4 --allow-run-as-root a.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4762996",
   "metadata": {},
   "source": [
    "***\n",
    "#### Python skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd25a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ring.py\n",
    "from mpi4py import rc\n",
    "rc.initialize = False\n",
    "rc.thread_level = 'single'\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "MPI.Init()\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "status = MPI.Status()\n",
    "\n",
    "right = (rank+1) % size\n",
    "left = (rank-1+size) % size\n",
    "sum = 0\n",
    "snd_buf = np.zeros(1, dtype='d')\n",
    "snd_buf[0] = rank\n",
    "rcv_buf = np.zeros(1, dtype='d')\n",
    "\n",
    "# Create the window\n",
    "win = MPI.Win.Create(__________)\n",
    "    \n",
    "for i in range (0,size):\n",
    "    request = comm.issend(snd_buf, dest=right)\n",
    "    rcv_buf = comm.recv(source=left)\n",
    "    request.wait()\n",
    "    \n",
    "    snd_buf = rcv_buf\n",
    "    sum += rcv_buf[0]\n",
    "\n",
    "print(\"PE%i:\\tSum = %i\" % (rank, sum))\n",
    "\n",
    "MPI.Finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3eb525",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -np 4 --allow-run-as-root python ring.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6057a2be",
   "metadata": {},
   "source": [
    "***\n",
    "#### Fortran skeleton\n",
    "Additional hints:\n",
    "\n",
    "1. In Fortran you must have variables because you are doing call by reference, not by value. \n",
    "~~~Fortran\n",
    "integer :: disp_unit\n",
    "integer(KIND=MPI_ADDRESS_KIND) :: rcv_buf_size, lb, extent\n",
    "! get the extent of the integer\n",
    "call MPI_Type_get_extent(MPI_INTEGER, lb, extent, error)\n",
    "...\n",
    "disp_unit = extent\n",
    "! multiplied by the number of elements\n",
    "rcv_buf_size = disp_unit * 1\n",
    "call MPI_Win_create(rcv_buf, rcv_buf_size, disp_unit, MPI_INFO_NULL, ..., error)\n",
    "~~~\n",
    "\n",
    "2. Both buffers must be asynchronous. Also write additional statements to protect the receive buffer before the first fence and after the second fence and protect the send buffer after the second fence. \n",
    "~~~Fortran\n",
    "if (.NOT.MPI_ASYNC_PROTECTS_NONBLOCKING) call MPI_F_sync_reg(snd_buf)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c52170",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ring.f90\n",
    "program ring\n",
    "use mpi\n",
    "\n",
    "integer ( kind = 4 ) error\n",
    "integer :: rank, size\n",
    "integer :: right, left\n",
    "integer :: i, sum\n",
    "integer, asynchronous :: snd_buf\n",
    "integer :: rcv_buf\n",
    "integer :: status(MPI_STATUS_SIZE)\n",
    "integer :: request\n",
    "_____________ :: win \n",
    "_____________ :: disp_unit\n",
    "_____________ :: extent, lb\n",
    "_____________ :: rcv_buf_size, target_disp\n",
    "\n",
    "call MPI_Init(error)\n",
    "call MPI_Comm_rank(MPI_COMM_WORLD, rank, error)\n",
    "call MPI_Comm_size(MPI_COMM_WORLD, size, error)\n",
    "\n",
    "right = mod(rank+1,      size)\n",
    "left  = mod(rank-1+size, size)\n",
    "\n",
    "! create the window\n",
    "call MPI_Type_get_extent(MPI_INTEGER, lb, extent, error)\n",
    "disp_unit = __________________\n",
    "rcv_buf_size = _______________\n",
    "call MPI_Win_create( _________________ , win, error) \n",
    "\n",
    "sum = 0\n",
    "snd_buf = rank\n",
    "\n",
    "do i = 1, size\n",
    "    call MPI_Issend(snd_buf, 1, MPI_INTEGER, right, 17, MPI_COMM_WORLD, request, error)\n",
    "    call MPI_Recv(rcv_buf, 1, MPI_INTEGER, left, 17, MPI_COMM_WORLD, status, error)\n",
    "    call MPI_Wait(request, status, error)\n",
    "    if (.NOT.MPI_ASYNC_PROTECTS_NONBLOCKING) call MPI_F_sync_reg(snd_buf)\n",
    "    snd_buf = rcv_buf\n",
    "    sum = sum + rcv_buf\n",
    "end do\n",
    "\n",
    "print *, 'PE', rank, ': Sum =', sum\n",
    "call MPI_Win_free(win, error)\n",
    "call MPI_Finalize(error)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7bac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpif90 ring.f90 && mpirun -np 4 --allow-run-as-root a.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e3ad59",
   "metadata": {},
   "source": [
    "***\n",
    "### You can compare with our solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670b288b",
   "metadata": {},
   "source": [
    "***\n",
    "#### C solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d30384",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ring.c\n",
    "#include <stdio.h>\n",
    "#include <mpi.h>\n",
    "\n",
    "int main()\n",
    "{\n",
    "    int rank, size;\n",
    "    int snd_buf, rcv_buf;\n",
    "    int right, left;\n",
    "    int sum, i;\n",
    "\n",
    "    MPI_Win win;\n",
    "\n",
    "    MPI_Init(NULL, NULL);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "\n",
    "    right = (rank+1)      % size;\n",
    "    left  = (rank-1+size) % size;\n",
    "\n",
    "    /* Create the window. */\n",
    "    MPI_Win_create(&rcv_buf, (MPI_Aint) sizeof(int), sizeof(int), MPI_INFO_NULL, MPI_COMM_WORLD, &win);\n",
    "\n",
    "    sum = 0;\n",
    "    snd_buf = rank;\n",
    "\n",
    "    for(i = 0; i < size; i++) \n",
    "    {\n",
    "        MPI_Win_fence(MPI_MODE_NOSTORE | MPI_MODE_NOPRECEDE, win);\n",
    "        MPI_Put(&snd_buf, 1, MPI_INT, right, (MPI_Aint) 0, 1, MPI_INT, win);\n",
    "        MPI_Win_fence(MPI_MODE_NOSTORE | MPI_MODE_NOPUT | MPI_MODE_NOSUCCEED, win);\n",
    "\n",
    "        snd_buf = rcv_buf;\n",
    "        sum += rcv_buf;\n",
    "    }\n",
    "\n",
    "    printf(\"PE%i:\\tSum = %i\\n\", rank, sum);\n",
    "\n",
    "    MPI_Win_free(&win);\n",
    "\n",
    "    MPI_Finalize();\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2402c500",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpicc ring.c && mpirun -np 4 --allow-run-as-root a.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb7d15d",
   "metadata": {},
   "source": [
    "***\n",
    "#### Python solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9bcaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ring.py\n",
    "from mpi4py import rc\n",
    "rc.initialize = False\n",
    "rc.thread_level = 'single'\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "MPI.Init()\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "status = MPI.Status()\n",
    "\n",
    "right = (rank+1) % size\n",
    "left = (rank-1+size) % size\n",
    "sum = 0\n",
    "snd_buf = np.zeros(1, dtype='d')\n",
    "snd_buf[0] = rank\n",
    "rcv_buf = np.zeros(1, dtype='d')\n",
    "\n",
    "# Create the window\n",
    "win = MPI.Win.Create(rcv_buf, 1, MPI.INFO_NULL, comm)\n",
    "    \n",
    "for i in range (0,size):\n",
    "    MPI.Win.Fence(win)\n",
    "    MPI.Win.Put(win, snd_buf, right)\n",
    "    MPI.Win.Fence(win)\n",
    "\n",
    "    snd_buf = rcv_buf\n",
    "    sum += rcv_buf[0]\n",
    "\n",
    "print(\"PE%i:\\tSum = %i\" % (rank, sum))\n",
    "\n",
    "MPI.Win.Free(win)\n",
    "MPI.Finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de8d6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -np 4 --allow-run-as-root python ring.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4c3cda",
   "metadata": {},
   "source": [
    "***\n",
    "#### Fortran solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a85ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ring.f90\n",
    "program ring\n",
    "use mpi\n",
    "\n",
    "integer ( kind = 4 ) error\n",
    "integer :: rank, size\n",
    "integer :: right, left\n",
    "integer :: i, sum\n",
    "\n",
    "! both buffers must be asynchronous\n",
    "integer, asynchronous :: snd_buf, rcv_buf\n",
    "integer :: win\n",
    "integer :: disp_unit\n",
    "integer(KIND=MPI_ADDRESS_KIND) :: extent, lb\n",
    "integer(KIND=MPI_ADDRESS_KIND) :: rcv_buf_size, target_disp\n",
    "\n",
    "call MPI_Init(error)\n",
    "call MPI_Comm_rank(MPI_COMM_WORLD, rank, error)\n",
    "call MPI_Comm_size(MPI_COMM_WORLD, size, error)\n",
    "\n",
    "right = mod(rank+1,      size)\n",
    "left  = mod(rank-1+size, size)\n",
    "\n",
    "! create the window\n",
    "call MPI_Type_get_extent(MPI_INTEGER, lb, extent, error)\n",
    "disp_unit = extent\n",
    "rcv_buf_size = 1 * disp_unit\n",
    "call MPI_Win_create(rcv_buf, rcv_buf_size, disp_unit, MPI_INFO_NULL, MPI_COMM_WORLD, win, error)\n",
    "\n",
    "sum = 0\n",
    "snd_buf = rank\n",
    "\n",
    "! protect the receive buffer before and after the two fences\n",
    "! protect the send buffer because after put and second fence, the buffer must not be modified\n",
    "do i = 1, size\n",
    "    if (.NOT.MPI_ASYNC_PROTECTS_NONBLOCKING) call MPI_F_sync_reg(rcv_buf)\n",
    "\n",
    "    call MPI_Win_fence(MPI_MODE_NOSTORE + MPI_MODE_NOPRECEDE, win, error)\n",
    "    target_disp = 0\n",
    "    call MPI_Put(snd_buf, 1, MPI_INTEGER, right, target_disp, 1, MPI_INTEGER, win, error)\n",
    "    call MPI_Win_fence(MPI_MODE_NOSTORE + MPI_MODE_NOPUT + MPI_MODE_NOSUCCEED, win, error)\n",
    "    \n",
    "    if (.NOT.MPI_ASYNC_PROTECTS_NONBLOCKING) CALL MPI_F_sync_reg(rcv_buf)\n",
    "\n",
    "    if (.NOT.MPI_ASYNC_PROTECTS_NONBLOCKING) CALL MPI_F_sync_reg(snd_buf)\n",
    "\n",
    "    snd_buf = rcv_buf\n",
    "    sum = sum + rcv_buf\n",
    "end do\n",
    "\n",
    "print *, 'PE', rank, ': Sum =', sum\n",
    "\n",
    "call MPI_Win_free(win, error)\n",
    "\n",
    "call MPI_Finalize(error)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef67a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpif90 ring.f90 && mpirun -np 4 --allow-run-as-root a.out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++14 with OpenMP and MPI",
   "language": "C++14",
   "name": "xcpp14"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
