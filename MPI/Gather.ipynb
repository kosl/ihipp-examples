{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f166b008",
   "metadata": {},
   "source": [
    "## Gather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b46418c",
   "metadata": {},
   "source": [
    "This provided skeleton first gathers the data into an array at process 0 and then process 0 prints the whole array. \n",
    "\n",
    "In the below skeleton you should substitute the point-to-point communication by one call to MPI_Gather, which means you have to remove the whole gathering loop. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d911644e",
   "metadata": {},
   "source": [
    "***\n",
    "#### C skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f14da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mpi.h>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fed415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%executable  a.x -- -lmpi\n",
    "\n",
    "int n; double result;\n",
    "double *result_array;\n",
    "int my_rank, num_procs, rank;\n",
    "\n",
    "MPI_Init(NULL, NULL); \n",
    "MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n",
    "MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n",
    "\n",
    "// doing some application work in each process, e.g.:\n",
    "result = 100.0 + 1.0 * my_rank;\n",
    "printf(\"I am process %i out of %i, result = %f \\n\", my_rank, num_procs, result);\n",
    "\n",
    "if (my_rank == 0) {\n",
    "    // memory allocation needed only on root process\n",
    "    result_array = (double *)malloc(sizeof(double) * num_procs);\n",
    "}\n",
    "\n",
    "// start of gathering\n",
    "if (my_rank != 0) { // sending some results from all processes (except 0) to process 0\n",
    "    MPI_Send(&result, 1, MPI_DOUBLE, 0, 99, MPI_COMM_WORLD);\n",
    "} else { // only in \"root\" process 0\n",
    "    result_array[0] = result; // process 0's own result\n",
    "    // receiving all these messages\n",
    "    for (rank=1; rank<num_procs; rank++)\n",
    "    { // result of processes 1, 2, ...\n",
    "        MPI_Recv(&result_array[rank], 1, MPI_DOUBLE, rank, 99, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "    }\n",
    "}\n",
    "// end of gathering\n",
    "\n",
    "if (my_rank == 0) {\n",
    "    for (rank=0; rank<num_procs; rank++)\n",
    "        printf(\"I'm proc 0: result of process %i is %f \\n\", rank, result_array[rank]); \n",
    "}\n",
    "\n",
    "MPI_Finalize();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4a1d57",
   "metadata": {},
   "source": [
    "Now compile it and run it with 4 processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36f8be0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mpirun -np 4 a.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d523ee",
   "metadata": {},
   "source": [
    "***\n",
    "#### Python skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7596fb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file gather.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "my_rank = comm.Get_rank()\n",
    "num_procs = comm.Get_size()\n",
    "    \n",
    "# doing some application work in each process, e.g.:\n",
    "result = 100.0 + 1.0 * my_rank\n",
    "print(\"I am process %i out of %i, result = %f\" % (my_rank, num_procs, result))\n",
    "\n",
    "if my_rank == 0:\n",
    "    # memory allocation needed only on root process\n",
    "    result_array = [None] * num_procs\n",
    "\n",
    "# start of gathering\n",
    "if my_rank != 0: # sending some results from all processes (except 0) to process 0\n",
    "    comm.send(result, dest=0)\n",
    "if my_rank == 0: # only in \"root\" process 0\n",
    "    result_array[0] = result # process 0's own result\n",
    "    # receiving all these messages\n",
    "    for rank in range (1,num_procs): # result of processes 1, 2, ...\n",
    "        result_array[rank] = comm.recv(source=rank)\n",
    "# end of gathering\n",
    "\n",
    "if my_rank == 0:\n",
    "    for rank in range(0,num_procs):\n",
    "        print(\"I am proc 0: result of process %i is %f\" % (rank, result_array[rank]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15f4b77",
   "metadata": {},
   "source": [
    "Now compile it and run it with 4 processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12057c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -np 4 python gather.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9936fe22",
   "metadata": {},
   "source": [
    "***\n",
    "#### Fortran skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcc496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file gather.f90\n",
    "program gather\n",
    "use mpi\n",
    "\n",
    "integer ( kind = 4 ) error\n",
    "integer :: n\n",
    "double precision :: result\n",
    "double precision, allocatable, dimension (:) :: result_array\n",
    "integer :: my_rank, num_procs, rank\n",
    "\n",
    "call MPI_Init(error)\n",
    "call MPI_Comm_rank(MPI_COMM_WORLD, my_rank, error)\n",
    "call MPI_Comm_size(MPI_COMM_WORLD, num_procs, error)\n",
    "\n",
    "! doing some application work in each process, e.g.:\n",
    "result = 100.0 + 1.0 * my_rank\n",
    "print *, 'I am process', my_rank, 'out of', num_procs, 'result = ', result\n",
    "\n",
    "if (my_rank == 0) then\n",
    "    ! memory allocation needed only on root process\n",
    "    allocate(result_array(0:num_procs-1))\n",
    "endif\n",
    "\n",
    "! start of gathering\n",
    "if (my_rank /= 0) then ! sending some results from all processes (except 0) to process 0\n",
    "    call MPI_Send(result, 1, MPI_DOUBLE_PRECISION, 0, 99, MPI_COMM_WORLD, error)\n",
    "else ! only in \"root\" process 0\n",
    "    result_array(0) = result ! process 0's own result\n",
    "    ! receiving all these messages\n",
    "    do rank = 1, num_procs-1 ! result of processes 1, 2, ...\n",
    "        call MPI_Recv(result_array(rank), 1, MPI_DOUBLE_PRECISION, rank, 99, MPI_COMM_WORLD, MPI_STATUS_IGNORE, error)\n",
    "    end do\n",
    "endif\n",
    "! end of gathering\n",
    "\n",
    "if (my_rank == 0) then\n",
    "    do rank = 0, num_procs-1\n",
    "        print *, \"I'm proc 0: result of process\", rank, 'is', result_array(rank)\n",
    "    end do\n",
    "endif\n",
    "\n",
    "call MPI_Finalize(error)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f56ea58",
   "metadata": {},
   "source": [
    "Now compile it and run it with 4 processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b2ab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpif90 gather.f90 && mpirun -np 4 a.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d6e30d",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeebe3d",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6088d5ce",
   "metadata": {},
   "source": [
    "### You can compare with our solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d3fdb7",
   "metadata": {},
   "source": [
    "***\n",
    "#### C solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9add07dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%executable  a.x -- -lmpi\n",
    "\n",
    "int n; double result;\n",
    "double *result_array;\n",
    "int my_rank, num_procs, rank;\n",
    "\n",
    "MPI_Init(NULL, NULL); \n",
    "MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n",
    "MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n",
    "\n",
    "// doing some application work in each process, e.g.:\n",
    "result = 100.0 + 1.0 * my_rank;\n",
    "printf(\"I am process %i out of %i, result = %f \\n\", my_rank, num_procs, result);\n",
    "\n",
    "if (my_rank == 0) {\n",
    "    // memory allocation needed only on root process\n",
    "    result_array = (double *)malloc(sizeof(double) * num_procs);\n",
    "}\n",
    "\n",
    "// start of gathering\n",
    "MPI_Gather(&result, 1, MPI_DOUBLE, result_array, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n",
    "// end of gathering\n",
    "\n",
    "if (my_rank == 0) {\n",
    "    for (rank=0; rank<num_procs; rank++)\n",
    "        printf(\"I'm proc 0: result of process %i is %f \\n\", rank, result_array[rank]); \n",
    "}\n",
    "\n",
    "MPI_Finalize();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc5bc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -np 4 a.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a467c8f8",
   "metadata": {},
   "source": [
    "***\n",
    "#### Python solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304966f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file gather.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "my_rank = comm.Get_rank()\n",
    "num_procs = comm.Get_size()\n",
    "    \n",
    "# doing some application work in each process, e.g.:\n",
    "result = 100.0 + 1.0 * my_rank\n",
    "print(\"I am process %i out of %i, result = %f\" % (my_rank, num_procs, result))\n",
    "\n",
    "if my_rank == 0:\n",
    "    # memory allocation needed only on root process\n",
    "    result_array = [None] * num_procs\n",
    "    \n",
    "# start of gathering\n",
    "result_array = comm.gather(result, root=0)\n",
    "# end of gathering\n",
    "\n",
    "if my_rank == 0:\n",
    "    for rank in range(0,num_procs):\n",
    "        print(\"I am proc 0: result of process %i is %f\" % (rank, result_array[rank]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb648087",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -np 4 python gather.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6976409a",
   "metadata": {},
   "source": [
    "***\n",
    "#### Fortran solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86652edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file gather.f90\n",
    "program gather\n",
    "use mpi\n",
    "\n",
    "integer ( kind = 4 ) error\n",
    "integer :: n\n",
    "double precision :: result\n",
    "double precision, allocatable, dimension (:) :: result_array\n",
    "integer :: my_rank, num_procs, rank\n",
    "\n",
    "call MPI_Init(error)\n",
    "call MPI_Comm_rank(MPI_COMM_WORLD, my_rank, error)\n",
    "call MPI_Comm_size(MPI_COMM_WORLD, num_procs, error)\n",
    "\n",
    "! doing some application work in each process, e.g.:\n",
    "result = 100.0 + 1.0 * my_rank\n",
    "print *, 'I am process', my_rank, 'out of', num_procs, 'result = ', result\n",
    "\n",
    "if (my_rank == 0) then\n",
    "    ! memory allocation needed only on root process\n",
    "    allocate(result_array(0:num_procs-1))\n",
    "endif\n",
    "\n",
    "! start of gathering\n",
    "call MPI_Gather(result, 1, MPI_DOUBLE_PRECISION, result_array, 1, MPI_DOUBLE_PRECISION, 0, MPI_COMM_WORLD, error)\n",
    "! end of gathering\n",
    "\n",
    "if (my_rank == 0) then\n",
    "    do rank = 0, num_procs-1\n",
    "        print *, \"I'm proc 0: result of process\", rank, 'is', result_array(rank)\n",
    "    end do\n",
    "endif\n",
    "\n",
    "call MPI_Finalize(error)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d4acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpif90 gather.f90 && mpirun -np 4 a.out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++14 with OpenMP and MPI",
   "language": "C++14",
   "name": "xcpp14"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
